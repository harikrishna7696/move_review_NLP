{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832d4fa9-829b-4486-93bf-73c5f5057f30",
   "metadata": {},
   "source": [
    "##  Movie Review project with nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121e47de-837b-40af-b35a-0bdb1da503a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import movie_reviews, stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d858f7ea-e65e-4433-ab94-f5ec93faccc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.NOUN # it return the single string that understand by wordnetlemmitazier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ee808aa-d211-4229-9457-9e7070bb36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this function take the tag of which returned by pos_tag function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0d213c4a-85f3-4464-a444-5c11015f6dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag): \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c2683cae-d222-4453-8e8e-015bb1997b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories() # checking the categories of the movie_riviews datastet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a1af14fe-524e-41f6-8963-c5835d21fe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = movie_reviews.fileids() #it conaing 1000 pos and 1000 neg\n",
    "len(data) #pos --> Positive, neg --> Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "daeaee40-247a-45f4-b90d-b4818b5e33d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_reviews.words(data[:1]))) # getting the words from the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a1645603-cb03-4c32-8258-8a4c1b4381d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In cleaning process we need to remove the stopping words from documents\n",
    "stop_words = stopwords.words('english') # nltk has inbuilt list that contain stop words \n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "570cbaa1-f16d-47e5-8f31-2f0687e2626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "stop_words += list(string.punctuation) # we have to add the punctuation to stop_words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "546180cb-a062-4d66-b87a-431f26fce014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12b8bf65-4881-479a-9f57-df8bf70a484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f87acf-1701-4a69-8e65-8aa87a1b848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting words and categories into 1 file\n",
    "def get_document_movie(shuffle=True): # here the suffle parameter act to shuffle the list or not \n",
    "    documents = [] # creating a empty list to store the words\n",
    "    for cat in movie_reviews.categories(): # extracting the categories \n",
    "        for doc in movie_reviews.fileids(cat): # based on category getting the txt files\n",
    "            documents.append((movie_reviews.words(doc), cat)) #append the tuple(words, category) to list\n",
    "    if shuffle: # check the condition if it's true it suffle the documents else it return as it is\n",
    "        random.shuffle(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "28f4740a-2335-4d2c-9dce-54d7938db25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_document_movie() # this function return the all documents you can find above this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "71ac8104-1f73-4d69-b1f4-a8698da0ba82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['to', 'put', 'it', 'bluntly', ',', 'ed', 'wood', ...], 'neg'),\n",
       " (['the', 'question', 'isn', \"'\", 't', 'why', 'has', ...], 'pos')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ac1be34b-3116-4d3f-a381-ee9de44977c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['hari'])[0][1] # demonstation of getting part of speech tag from pos_tag method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef677a1c-ecb7-4e86-8dc3-de7dd30cdb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmit = WordNetLemmatizer() # creating a object of lemmitizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b092ae02-74c0-42c6-982e-6b83087f59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function clean the words and return the useful words based on pos\n",
    "def clean_words(words): # words-> list of words \n",
    "    output_words = [] # appending the all words after cleaning \n",
    "    for w in words: \n",
    "        if w.lower() not in stop_words: # converting the word into lower and checking for the stop words\n",
    "            pos = pos_tag([w])[0][1] # pos_tag([word])[tuple][tag]\n",
    "            clean_word = lemmit.lemmatize(w, pos = get_simple_pos(pos)) # lemmatize(word, pos=part of speech)\n",
    "            output_words.append(clean_word.lower()) # after cleaning word appending the word into list\n",
    "    return output_words  # returning the cleaned words list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dbd72105-b8c7-428c-a531-e8d17192068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_document = [(clean_words(word), cat) for word, cat in documents] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c90c67c2-c9c7-4db4-8235-8a5942df3549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['dr', 'alan', 'grant', 'sam', 'neill', 'jurassic', 'park', 'become', 'disillusion', 'paleontology', 'longer', 'sexy', 'science', 'since', 'ingen', 'corporation', 'clone', 'subject', 'matter', 'lecture', 'bring', 'people', 'interested', 'adventure', 'isla', 'nubla', 'rather', 'research', 'funding', 'dollar', 'dry', 'kirbys', 'william', 'h', 'macy', 'fargo', 'tea', 'leoni', 'family', 'man', 'ask', 'guide', 'anniversary', 'flyover', 'isla', 'sorna', 'notorious', 'site', 'b', 'lose', 'world', 'disdainful', 'wave', 'checkbook', 'reconsiders', 'however', 'kirbys', 'give', 'dr', 'grant', 'real', 'agenda', 'jurassic', 'park', 'iii', 'course', 'audience', 'tipped', 'give', 'film', 'begin', 'show', 'u', 'eric', 'trevor', 'morgan', 'patriot', 'young', 'boy', 'ben', 'mark', 'harelik', 'election', 'go', 'paragliding', 'adventure', 'island', 'go', 'awry', 'look', 'like', 'cheesy', 'rear', 'projection', 'grant', 'establish', 'back', 'home', 'new', 'right', 'hand', 'man', 'billy', 'brennan', 'alessandro', 'nivola', 'love', 'labour', 'lose', 'site', 'dig', 'montana', 'sorely', 'lack', 'fund', 'also', 'pay', 'visit', 'old', 'flame', 'dr', 'ellie', 'sattler', 'laura', 'dern', 'jurassic', 'park', 'married', 'another', 'young', 'son', 'call', 'grant', 'dinosaur', 'man', 'apparently', 'sole', 'purpose', 'dredge', 'film', 'poorly', 'imagine', 'finale', 'grant', 'take', 'billy', 'along', 'kirbys', 'trip', 'really', 'illegal', 'gambit', 'save', 'son', 'young', 'paraglider', 'couple', 'millionaire', 'make', 'grant', 'check', 'bogus', 'separate', 'well', 'eric', 'amanda', 'new', 'boyfriend', 'make', 'much', 'sense', 'meaning', 'gooey', 'family', 'dynamic', 'wait', 'dino', 'din', 'kirbys', 'hire', 'hand', 'obvious', 'bait', 'threesome', 'lead', 'mr', 'udesky', 'michael', 'jeter', 'gift', 'anyone', 'consider', 'cast', 'michael', 'jeter', 'william', 'h', 'macy', 'together', 'related', 'little', 'odd', 'direct', 'joe', 'johnston', 'october', 'sky', 'jumanji', 'spielberg', 'produce', 'one', 'risible', 'script', 'peter', 'buchman', 'election', 'team', 'alexander', 'payne', 'jim', 'taylor', 'jurassic', 'park', 'iii', 'nothing', 'quickie', 'monster', 'flick', 'couple', 'new', 'dinos', 'spinosauraus', 'go', 'head', 'head', 'rex', 'pteranodons', 'plot', 'series', 'coincidence', 'combine', 'extreme', 'leap', 'faith', 'trifecta', 'stupid', 'cell', 'phone', 'trick', 'effect', 'longer', 'new', 'shot', 'television', 'cinematographer', 'shelly', 'johnson', 'rather', 'murky', 'look', 'time', 'film', 'edit', 'robert', 'dalva', 'october', 'sky', 'presumably', 'do', 'machete', 'keep', '90', 'minute', 'run', 'time', 'know', 'reason', 'explain', 'ridiculous', 'end', 'feature', 'survivor', 'confront', 'pack', 'raptor', 'save', 'ludicrous', 'logic', 'jump', 'within', 'minute', 'original', 'music', 'davis', 'repeat', 'john', 'williams', 'original', 'theme', 'neill', 'young', 'morgan', 'attempt', 'inject', 'humor', 'humanity', 'proceeding', 'rest', 'cast', 'plod', 'unexceptional', 'jurassic', 'park', 'iii', 'probably', 'provide', 'quick', 'entertainment', 'go', 'know', 'expect', 'crowd', 'maybe', 'like', 'lose', 'world', 'jurassic', 'park'], 'neg')]\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_document[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "62a9e388-20fe-4100-b60a-1cc663fe2c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_document = cleaned_document[:1500] # training and testing data for nltk classifier\n",
    "testing_document = cleaned_document[1500:]\n",
    "# nltk need classifier of list that need to be tuple of words and cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0b904bba-5ad3-4e4c-88c3-ba066b929095",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for word in training_document:\n",
    "    all_words += word[0] # extracting all words from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7a6b7ab9-4838-447f-b867-6e6785b86a77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 8350), ('movie', 5181), ('one', 4479), ('make', 3269), ('like', 3014), ('character', 2884), ('get', 2810), ('see', 2349), ('go', 2245), ('time', 2234), ('well', 2214), ('even', 1986), ('scene', 1978), ('good', 1824), ('story', 1725), ('take', 1646), ('would', 1588), ('much', 1588), ('come', 1498), ('bad', 1479), ('also', 1456), ('life', 1446), ('give', 1443), ('two', 1429), ('look', 1413), ('know', 1395), ('way', 1387), ('end', 1385), ('seem', 1382), ('first', 1373), ('year', 1321), ('--', 1314), ('work', 1310), ('say', 1232), ('thing', 1230), ('plot', 1202), ('really', 1188), ('little', 1163), ('play', 1159), ('people', 1152), ('show', 1142), ('could', 1116), ('star', 1072), ('love', 1051), ('man', 1043), ('great', 1006), ('big', 1000), ('never', 999), ('performance', 996), ('try', 995)]\n"
     ]
    }
   ],
   "source": [
    "freq = nltk.FreqDist(all_words) # using FreqDist() we can find the word frequency\n",
    "print(freq.most_common(50))  # getting most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "29a14e17-4cec-45ba-8a3c-e1874cc181d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dict(document):\n",
    "    freq_dict = []\n",
    "    for words, cat in document:\n",
    "        freq_dict.append((nltk.FreqDist(words), cat))\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ef13d575-d90f-4b23-8477-cf8435e6f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(all_words, how_many=200):\n",
    "    freq = nltk.FreqDist(all_words)\n",
    "    most = freq.most_common(how_many)\n",
    "    return [i[0] for i in most]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d86e60d1-95b2-4715-846e-1a95df2ff0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = get_features(all_words, how_many=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "531147e8-1366-4b1d-ab14-b9d4865e492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'movie', 'one', 'make', 'like', 'character', 'get', 'see', 'go', 'time', 'well', 'even', 'scene', 'good', 'story', 'take', 'would', 'much', 'come', 'bad', 'also', 'life', 'give', 'two', 'look', 'know', 'way', 'end', 'seem', 'first', 'year', '--', 'work', 'say', 'thing', 'plot', 'really', 'little', 'play', 'people', 'show', 'could', 'star', 'love', 'man', 'great', 'big', 'never', 'performance', 'try', 'director', 'best', 'want', 'new', 'many', 'actor', 'action', 'think', 'find', 'watch', 'u', 'role', 'act', 'another', 'still', 'audience', 'back', 'turn', 'something', 'world', 'day', 'however', 'old', 'set', 'though', 'every', 'feel', 'guy', 'comedy', 'begin', 'use', 'real', 'enough', 'around', 'part', 'cast', 'last', 'point', 'may', 'interest', 'run', 'write', 'woman', 'young', 'name', 'actually', 'john', 'lot', 'script', 'funny']\n"
     ]
    }
   ],
   "source": [
    "print(features[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f64be0f1-1873-4482-bc8a-4ea7931efd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dict(words):\n",
    "    current_features = {}\n",
    "    word_set = set(words)\n",
    "    for w in features:\n",
    "        current_features[w] = w in word_set\n",
    "    return current_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9368b2ad-3757-4dd8-b23a-869c91d5cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(get_features_dict(words), cat) for words, cat in training_document]\n",
    "testing_data = [(get_features_dict(words), cat) for words, cat in testing_document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8918efc7-cf8b-4193-81ed-fd136acf5e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 500)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data), len(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53460ca8-154c-4dac-a434-2d7ca6a5a248",
   "metadata": {
    "tags": []
   },
   "source": [
    "## classification using nltk naive bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "952d787d-a1d4-448c-a6e5-0c891846cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eeef2898-03b2-4846-a772-5d026ecf861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "88367bb4-9cfd-4a48-952c-927162d25ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "84ea3fdd-c55a-4e98-9c87-71471a832706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.758"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f8d6c-560e-4ea7-b23c-96dc54049189",
   "metadata": {},
   "source": [
    "## Using sklear classifiers with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8a0df619-9f7a-4fcf-9e5b-1f4032c3f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 suppose = True              neg : pos    =      2.1 : 1.0\n",
      "           unfortunately = True              neg : pos    =      1.9 : 1.0\n",
      "                     war = True              pos : neg    =      1.9 : 1.0\n",
      "                     bad = True              neg : pos    =      1.9 : 1.0\n",
      "                     bad = False             pos : neg    =      1.8 : 1.0\n",
      "                    true = True              pos : neg    =      1.8 : 1.0\n",
      "                american = True              pos : neg    =      1.7 : 1.0\n",
      "                    joke = True              neg : pos    =      1.7 : 1.0\n",
      "              especially = True              pos : neg    =      1.7 : 1.0\n",
      "                  scream = True              neg : pos    =      1.7 : 1.0\n",
      "               different = True              pos : neg    =      1.6 : 1.0\n",
      "                    save = True              neg : pos    =      1.6 : 1.0\n",
      "                 present = True              pos : neg    =      1.6 : 1.0\n",
      "                  minute = True              neg : pos    =      1.6 : 1.0\n",
      "                     car = True              neg : pos    =      1.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7a3b8c01-79bf-4ebd-b289-372e5b82a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a996344c-e368-448a-9f70-8c9020473440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=0.9)\n",
    "classifier = SklearnClassifier(svc)\n",
    "classifier.train(training_data)\n",
    "nltk.classify.accuracy(classifier, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3b823e0b-f424-45fc-b013-1e0971e95e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = {'hey this is hari krishna', 'this a data scientis krishna'}\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer(max_features=3)\n",
    "a = vec.fit_transform(train)\n",
    "a.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "712cbcb6-9b04-412f-8971-eeee7c58de33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data', 'krishna', 'this'], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4f75aa63-6f58-46a8-84de-553266d4aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [cat for word, cat in cleaned_document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c11a36fa-a0d4-46bb-94a0-13c7e097e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\" \".join(word) for word, cat in cleaned_document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a9763491-2243-49b2-a518-97e265a114e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3b24f5ac-729c-468b-b675-f22c82a422f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8df7015c-52e4-49c7-ae99-d70c505240c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()\n",
    "X_train_vector = vector.fit_transform(X_train)\n",
    "X_test_vector = vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4da61155-c6aa-42b7-acfa-b1cbc0e6aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '007', ..., 'zwigoff', 'zycie', 'zzzzzzz'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.get_feature_names_out(X_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "cbf35ccb-495b-4093-9035-1e63c88d7c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=225,bootstrap=False,  min_samples_leaf=7, random_state=52, )\n",
    "clf.fit(X_train_vector, Y_train)\n",
    "score = clf.score(X_test_vector, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "908e16d3-2d57-43d3-ba05-a39003db46bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = clf.predict(X_test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5380ea74-c8f2-4c06-aa88-494a656d94d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2782e05f-9de8-49eb-955d-376ad33a3006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[247  35]\n",
      " [ 70 248]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "50de96e9-7165-4fd4-96c3-6f41de32821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.78      0.88      0.82       282\n",
      "         pos       0.88      0.78      0.83       318\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.83      0.83      0.82       600\n",
      "weighted avg       0.83      0.82      0.83       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
